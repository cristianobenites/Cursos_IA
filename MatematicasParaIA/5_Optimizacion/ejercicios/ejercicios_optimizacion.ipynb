{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/MatematicasParaIA/5_Optimizacion/ejercicios/ejercicios_optimizacion.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Método de la bisección\n",
    "\n",
    "Implementar el método de la bisección. Debe tomar como argumentos, la función a evaluar, el intervalo inicial, y los criterios de corte. Debe devolver el valor  hallado.\n",
    "\n",
    "Recordar que la formulación del método tiene como objetivo encontrar $\\color{orange}{x}$ que iguale a cero una ecuación.\n",
    "\n",
    "Hint: usar como base el pseudocódigo provisto en la teórica\n",
    "\n",
    "<center>\n",
    "<img src=\"http://www.frsn.utn.edu.ar/gie/an/enl/Metodobiseccion.gif\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completa tu codigo aca\n",
    "\n",
    "def bisection(f, interval0, MAXITER, TOL):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2:\n",
    "\n",
    "Usar el método de la bisección para resolver el siguiente problema \n",
    "\n",
    "$$\n",
    "\\color{orange}{x^6-4x^5+\\sin (x)-e^x+\\pi =0}\n",
    "$$\n",
    "\n",
    "Graficar la función en el intervalo $\\color{orange}{[-1,1]}$ y usar este intervalo como el inicial \n",
    "\n",
    "Usar `numpy`  y `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completa tu código acá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "Modificar el método `fit` de la clase `LinearRegression` para poder guardar el historial del valores de la función error. Graficar el error en función del tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completa tu código acá\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, add_bias=True): \n",
    "        self.add_bias = add_bias #el bias es la constante de nuestro modelo lineal\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        \n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)]) \n",
    "        \n",
    "        N,D = x.shape\n",
    "        \n",
    "        def gradient(x, y, betas):                          \n",
    "            yh =  x @ betas # la operación calcula el producto matricial\n",
    "            N, D = x.shape \n",
    "            grad = -2*np.dot(y-yh, x)/N\n",
    "            return grad\n",
    "        \n",
    "        betas0 = np.zeros(D) #betas iniciales                                \n",
    "        self.betas = optimizer.run(gradient, x, y, betas0) #corremos el optimizador  \n",
    "        \n",
    "        #####\n",
    "        \n",
    "        def MSE(y,yh):\n",
    "            e = (y-yh)**2\n",
    "            return np.mean(e)\n",
    "        \n",
    "        def get_error_history(x,y,history_betas):\n",
    "            error_history = []\n",
    "            for betas in history_betas:\n",
    "                yh = x@betas\n",
    "                error_history.append(MSE(y,yh))\n",
    "            return error_history\n",
    "        \n",
    "        self.error_history = get_error_history(x,y,optimizer.w_history)        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias: \n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = x@self.betas\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta celda permanece igual \n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "        \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, record_history=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Inicializador\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        learning_rate (float, opcional): Ritmo de aprendizaje. Default a .001\n",
    "        max_iters (int, opcional): Máximas iteraciones. Default a 1E4.\n",
    "        epsilon (float, opcional): Cóta mínimma para el valor del gradiente permitido. Default a 1E-8\n",
    "        record_history (bool, opcional): Si es True guarda el historial de los pesos para cada paso de iteración. Default a False\n",
    "        \"\"\"\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        if record_history:\n",
    "            self.w_history = []                 \n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        \"\"\"\n",
    "        Método iterativo de gradiente descendiente. \n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        gradient_fn (function): gradiente de la función costo a minimizar. Debe tomar como parámetros x,y y los pesos correspondientes.\n",
    "        x (numpy.ndarray): variables independientes del conjunto de datos.\n",
    "        y (numpy.ndarray): variables dependientes observadas\n",
    "        w (numpy.ndarray): variables a entrenar del modelo \n",
    "\n",
    "        Devuelve\n",
    "        --------\n",
    "            w (numpy.ndarray): variables resultaantes luego del proceso de entrenamiento.\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = np.inf # definimos el gradiente como infinito para que se cumpla la condicion y se ingresse al while\n",
    "        t = 1 # contador\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters: #linalg.norm calcula la norma  L2 del vector gradiente\n",
    "            grad = gradient_fn(x, y, w) #calculamos el gradiente               \n",
    "            w = w - self.learning_rate * grad #acá esta la papa        \n",
    "            if self.record_history: \n",
    "                self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b0_true = 5\n",
    "b1_true = 0.7\n",
    "x_rl = np.linspace(-1,1,100)\n",
    "y_rl = x_rl*b1_true + b0_true + np.random.random(x_rl.shape)\n",
    "\n",
    "optimizer = GradientDescent(learning_rate=.05, max_iters=100, record_history=True) \n",
    "model = LinearRegression() #inicializamos nuestro objeto regresion \n",
    "model.fit(x_rl,y_rl, optimizer) #entrenamos el modelo\n",
    "\n",
    "_ = plt.figure(figsize=(7,7))\n",
    "\n",
    "#completar el plot\n",
    "# _ = plt.plot(..., '.')\n",
    "\n",
    "_ = plt.ylabel('MSE', fontsize=14)\n",
    "_ = plt.xlabel(\"Número de Iteración\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "Modificar el método  `run` de la clase `GradientDescent` para guardar el valor del gradiente en cada paso del entrenamiento. Graficar la evolución del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completa tu código acá\n",
    "\n",
    "class GradientDescent:\n",
    "        \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, record_history=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Inicializador\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        learning_rate (float, opcional): Ritmo de aprendizaje. Default a .001\n",
    "        max_iters (int, opcional): Máximas iteraciones. Default a 1E4.\n",
    "        epsilon (float, opcional): Cóta mínimma para el valor del gradiente permitido. Default a 1E-8\n",
    "        record_history (bool, opcional): Si es True guarda el historial de los pesos para cada paso de iteración. Default a False\n",
    "        \"\"\"\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        if record_history:\n",
    "            self.w_history = []\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        \"\"\"\n",
    "        Método iterativo de gradiente descendiente. \n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        gradient_fn (function): gradiente de la función costo a minimizar. Debe tomar como parámetros x,y y los pesos correspondientes.\n",
    "        x (numpy.ndarray): variables independientes del conjunto de datos.\n",
    "        y (numpy.ndarray): variables dependientes observadas\n",
    "        w (numpy.ndarray): variables a entrenar del modelo \n",
    "\n",
    "        Devuelve\n",
    "        --------\n",
    "            w (numpy.ndarray): variables resultaantes luego del proceso de entrenamiento.\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = np.inf # definimos el gradiente como infinito para que se cumpla la condicion y se ingresse al while\n",
    "        t = 1 # contador\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters: #linalg.norm calcula la norma  L2 del vector gradiente\n",
    "            grad = gradient_fn(x, y, w) #calculamos el gradiente               \n",
    "            w = w - self.learning_rate * grad #acá esta la papa        \n",
    "            if self.record_history: \n",
    "                self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_true = 5\n",
    "b1_true = 0.7\n",
    "x_rl = np.linspace(-1,1,100)\n",
    "y_rl = x_rl*b1_true + b0_true + np.random.random(x_rl.shape)\n",
    "\n",
    "optimizer = GradientDescent(learning_rate=.05, max_iters=100, record_history=True) \n",
    "model = LinearRegression() #inicializamos nuestro objeto regresion \n",
    "model.fit(x_rl,y_rl, optimizer) #entrenamos el modelo\n",
    "\n",
    "_ = plt.figure(figsize=(7,7))\n",
    "\n",
    "#completa tu código aca para graficar\n",
    "# _ = plt.plot(...  , '.')\n",
    "\n",
    "_ = plt.ylabel('Norma del gradiente', fontsize=14)\n",
    "_ = plt.xlabel(\"Número de Iteración\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "Implementar  un método que determine si la figura geométrica de un disco de radio 1 es convexa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1\n",
    "\n",
    "Implementar una función que permita determinar si un punto pertenece al conjunto definido como Disco.\n",
    "\n",
    "$$\n",
    "\\color{orange}{D = \\{(x,y) \\ : \\ \\|(x,y)\\|_2 \\leq 1 \\}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_disk(x, y, radius=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2\n",
    "\n",
    "Implementar una función que dos puntos, y construya una recta (la recta va a ser una serie de puntos pertenecientes a la recta). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completa tu código acá\n",
    "\n",
    "def build_line(x1, y1, x2, y2, NUM_POINTS=100):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3\n",
    "\n",
    "Implementar el método que chequee si los puntos pertenecientes a la recta pertenecen al conjunto disco. Usar puntos aleatorios asegurandose que caigan dentro de un círculo de radio 1. Esto es fácil usando coordenadas polares. Más info <a href=\"https://es.wikipedia.org/wiki/Coordenadas_polares\"> acá</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completa tu código acá\n",
    "def is_disk_convex(NUM_POINTS = 100):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
